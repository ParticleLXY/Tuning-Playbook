# 深度学习调参指南

## 1. 这份文档的受众

这份文档的受众是那些**想让他们的深度学习模型获得最佳性能**的工程人员和研究人员。我们假设读者已经掌握了基本的机器学习和深度学习的知识。

文档的重点是**模型超参数的优化**。我们也谈及了深度学习训练的其他方面，如流水线的部署和优化，但我们不打算深入这些问题。

我们假设文中机器学习处理的任务都是监督学习任务，或看起来像监督学习的任务（如自监督学习）。也就是说，文中提及的某些策略也适用于其他类型的任务。

## 2. 开始一个新项目的指南

在调参过程中，很多决定只需要在项目开始时定下，并且在情况发生变化时才会偶尔考虑修改。

我们的指南基于以下几个前提：

- 明确任务、数据清洗等准备工作已经完成，花点时间在模型的结构和训练的配置上是有意义的。
- 已有一个用于训练和评估的流水线，并且这个流水线适用于多个想要测试的模型。
- 选择恰当的评价指标，这个指标应该尽可能贴近部署的环境。

### 2.1 选择模型结构

太长不看：当开始一个新的工程时，首先尝试一个已经验证过的模型。

- 首先选择一个完善的、常用的模型结构开始你的工作。以后总有机会搭建一个自定义模型。
- 一个模型结构具有各种超参数来确定它的大小和其他细节（如层数、层宽、激活函数的类型等）
  - 因此，选择一个结构意味着选择了一系列不同的模型（根据模型超参数的不同）
  - 在xx和xx中描述了如何选择模型的超参数
- 如果可能，尝试找到一篇尽可能接近你所进行项目的论文并以这篇论文的模型为起点并复现它。

### 2.2 选择优化器

太长不看：以一个针对当前项目最常用的优化器开始。

- 没有一个面向所有的机器学习问题和模型结构的最好的优化器。比较不同优化器的性能是一项艰巨的任务。
- 我们建议坚持使用成熟的、流行的优化器，尤其是开始一个新项目时。
  - 理想情况下，选择用于同一类问题的最流行的优化器。
- 准备好关注所选优化器的所有超参数
  - 拥有越多超参数的优化器可能需要更多次的调参来找到最好的配置。
  - 尝试去找到其他超参数（如模型结构的超参数）的最优值而把优化器的超参数当作nuisance参数，尤其是在项目的开始阶段。
  - 在项目的初期，最好选择一个简单的优化器（如固定动量的SGD或固定$\epsilon$、$\beta_1$、$\beta_2$的Adam），在之后再去选择一个更适用的优化器。
- 我们喜欢的优化器（包括但不限于）：
  - 带动量的SGD（我们喜欢Nesterov变体）
  - Adam和NAdam，他们带有动量，而且比SGD更通用。Adam有4个超参数，而且它们都很重要！
    - 请参阅如何调整Adam的超参数

### 2.3 选择batch size

太长不看：batch size控制着训练速度，不应该直接用来调整验证集的性能。通常，理想的batch size是硬件支持的最大batch size。

- batch size是影响训练速度和计算资源消耗的关键因素。
- 增大batch size通常能缩短训练时间，这是非常有益的，因为：
  - 允许在有限时间里更充分地调参，最终的模型性能可能会更好。
  - 减少了开发周期的延迟，允许更频繁地实验新想法。
- 增加batch size可能会减少、增加或不改变资源消耗。
- batch size不应被视为一个影响验证集性能的可调超参数。
  - 只要所有的超参数都经过良好的调整（尤其是学习率和正则化），并且迭代次数足够，任意大小的batch size都将有相同的最终性能。
  - 请参阅为什么batch size不应被调整来改进验证集性能。

#### 2.3.1 确定可行的batch size并评估训练吞吐量(training throughput)

- 对一个给定的模型和优化器，硬件支持一系列的batch size。限制batch size的通常是显存。
- 不幸的是，如果不运行，或至少编译完整的训练程序，我们很难计算出合适的batch size。
- 最简单的解决方案是执行一系列batch size不同（通常以2的幂次增加）的训练任务，直到显存溢出。
- 对于每个batch size，我们应该训练足够长的时间来获得训练吞吐量的可靠估计

<center>训练吞吐量 = 每秒处理的样本数</center>

<center>也等价于每个训练步消耗的时间(time per step)</center>

<center>time per step = batch size / training throughput</center>

- 显存没有饱和的情况下，如果batch size增加一倍，训练吞吐量也会增加一倍，time per step基本保持不变。
- 如果不是上述的情况，则说明训练流水线存在瓶颈，如I/O或计算节点之间的同步。对此值得在继续之前对其进行诊断和纠正。
- 如果训练吞吐量在某个batch size后就不再增加，即使显存支持更大的batch size，我们也只能保持当前的batch size。
  - 只有增加batch size能带来吞吐量的增加，增大batch size才是有益的。如果没能增加吞吐量，去解决限制其的瓶颈或使用更小的batch size
  - Gradient accumulation模拟了比硬件支持的更大的batch size，但不能带来吞吐量上的任何好处，因此在工作中应该尽可能的避免。
- 在更改了模型或优化器之后，都需要重复上述的步骤。

#### 2.3.2 选择使训练时间最短的batch size

<center>Training time = (time per step) * (total number of steps)</center>

- 对于所有可行的batch size，在没有并行计算的开销和训练瓶颈时，我们一般假设time per step相等。实际上，增加batch size会带来一些额外的开销。
- 随着batch size的增加，达到特定性能所需要的training steps会减少（前提是修改batch size后调整所有相关的超参数）。
  - 完美缩放：batch size增加一倍，training steps减半。
  - 完美缩放适用于临界batch size下的所有batch size，超过临界batch size，收益会递减。
  - 最终，增加batch size不再减少training steps（但永远不会增加）。
- 因此，最小化训练时间的方法一般是采用临界batch size。

  - 临界batch size的大小取决于数据集、模型和优化器，如何通过计算得到它仍然是一个悬而未决的问题。
  - 当比较batch size时，注意区分一个样本的消耗、一个epoch的消耗和一个step的消耗之间的区别。
    - 只在完美缩放的范围内比较不同batch size下的epoch消耗，即使更大的batch size仍能缩短训练时间。
  - 通常情况下，硬件支持的最大batch size也小于临界batch size。所以一个通用准则是，尽可能选择最大的batch size。
- 不应该使用一个会增加训练时间的batch size。

#### 2.3.3 选择资源消耗最小的batch size

- 增大batch size会带来以下两类的资源消耗：
  1. 前期成本，如购买新硬件或重写训练流水线以支持多GPU/TPU。
  2. 使用成本，如团队预算，云服务的费用，电费，维护成本。



#### 2.3.4 修改batch size后需要对大部分超参数进行重新调参

- 大部分超参数对batch size敏感，所以修改batch size后需要对他们进行重新调参。
- 优先级最高的是优化器超参数和正则化超参数，因为它们与batch size强相关。
- 在项目初期选择batch size时请注意，如果以后要改变batch size，为新的batch size调整超参数可能会十分困难，耗时且成本高昂。

#### 2.3.5 batch norm和batch size的交互关系

- batch norm很复杂，通常来说，计算统计数据的batch size与计算梯度的batch size不同，详情请见batch norm部分。

### 2.4 选择初始配置

- 在对超参数进行调参前，我们需要确定起点，包括：
  1. 模型的结构（如层数）
  2. 优化器超参数（如学习率）
  3. training steps
- 确定初始配置需要反复实验。
- 我们的指导原则是找到一个简单、相对快速、资源消耗相对较低的配置，从而得到一个合理的结果。
  - 简单意味着尽可能避免花里胡哨的东西，这些可以在后期再加进去，即使花里胡哨的东西在后来被证明是有用的。
    - 例如，初始配置为固定学习率而不是花哨的衰减策略。
  - 选择快速且消耗最少资源的初始配置将使超参数调参更加高效。
    - 例如，从一个小模型开始。
  - 合理的表现视问题而定，但至少意味着经过训练的模型在验证集上的性能比随机好得多（尽管它可能很糟糕而不值得部署）。
- 选择合适的training steps需要平衡以下几点：
  - 一方面，训练更多steps可以改进模型性能并让调参更容易。
  - 另一方面，更少的training steps会让训练进程更快，也占用更少的资源，进行更多轮的训练吗，从而让调参更有效率。此外，如果一开始选择了一个不合适的training step，中途改变也不是很容易，例如根据training step制定的学习率衰减策略。

## 3. 提高模型性能的科学方法



